# (PART) Foundations {-} 

# Names and values

```{r, include=FALSE}
library(lobstr)
```

## Binding basics

1. __<span style="color:red">Q</span>__: Explain the relationship between `a`, `b`, `c` and `d` in the following code:

    ```{r}
    a <- 1:10
    b <- a
    c <- b
    d <- 1:10
    ```
    
   __<span style="color:green">A</span>__: `a`, `b`, `c` point to the same object (with the same address in memory). This object has the value `1:10`. `d` points to a different object with the same value.

    ```{r}
    list_of_names <- list(a, b, c, d)
    obj_addrs(list_of_names)
    ```

<!-- ```{r} -->
<!-- # alternative code: -->
<!-- list(a = a, b = b, c = c, d = d) %>% -->
<!--   map_chr(obj_addr) -->
<!-- ``` -->

2. __<span style="color:red">Q</span>__: The following code accesses the mean function in multiple different ways. Do they all point to the same underlying function object? Verify with `lobstr::obj_addr()`.
    
    ```{r, eval = FALSE}
    mean
    base::mean
    get("mean")
    evalq(mean)
    match.fun("mean")
    ```
    
   __<span style="color:green">A</span>__: Yes, they point to the same object. We confirm this by looking at the address of the underlying function object.
       
    ```{r}
    mean_functions <- list(mean,
                           base::mean,
                           get("mean"),
                           evalq(mean),
                           match.fun("mean"))
    
    unique(obj_addr(mean_functions))
    ```
    
3. __<span style="color:red">Q</span>__: By default, base R data import functions, like `read.csv()`, will automatically convert non-syntactic names to syntactic names. Why might this be problematic? What option allows you to suppress this behaviour?
    
   __<span style="color:green">A</span>__: When automatic and implicit (name) conversion occurrs, the prediction of a scripts output will be more difficult. For example when R is used non-interactively and some data is read, transformed and written, than the output may not contain the same names as the original data source. This could introduce problems in downstream analysis. To avoid automatic name conversion set `check.names=FALSE`.
    
4. __<span style="color:red">Q</span>__: What rules does `make.names()` use to convert non-syntactic names into syntactic names?
    
   __<span style="color:green">A</span>__: A valid name starts with a letter or a dot (which must not be followed by a number). It also consists of letters, numbers, dots and underscores only (`"_"` are allowed since R version 1.9.0).
   
   There are three main mechanisms to ensure syntactically valid names (see `?make.names`):
   - The variable name will be prepended by an `X` when names do not start with a letter or start with a dot followed by a number:
    
    ```{r}
    make.names("")
    make.names(".1")
    ```
    
   - (additionally) non-valid characters are replaced by a dot:
    
    ```{r}
    make.names("@")          # prepending + . replacement 
    make.names("  ")         # prepending + .. replacement
    make.names("non-valid")  # . replacement
    ```
    
   - reserved R keywords (see `?reserved`) are appended by a dot:
    
    ```{r}
    make.names("if")
    ```
    
   Interestingly, these some of these transformations may also depend on the current locale (see `?make.names`):

   > The definition of a letter depends on the current locale, but only ASCII digits are considered to be digits.

5. __<span style="color:red">Q</span>__: I slightly simplified the rules that govern syntactic names. Why is `.123e1` not a syntactic name? Read `?make.names` for the full details.
    
   __<span style="color:green">A</span>__: `.123e1` is not a syntact name, because it starts with one dot which is followed by a number.

## Copy-on-modify

1. __<span style="color:red">Q</span>__: Why is `tracemem(1:10)` not useful?

   __<span style="color:green">A</span>__: Without a binding `1:10` will not stay in memory (there will be no reference) and it makes no sense to track an object for copies which doesn't exist. Also when we assign `1:10` to a name, it will be clear, that `1:10` will only be the value of the object created and there is no "general" object `1:10`, which one would wan't to track.

2. __<span style="color:red">Q</span>__: Explain why `tracemem()` shows two copies when you run this code. 
   Hint: carefully look at the difference between this code and the code show earlier in the section.
     
    ```{r, results = FALSE}
    x <- c(1L, 2L, 3L)
    tracemem(x)
    
    x[[3]] <- 4
    ```
    
   __<span style="color:green">A</span>__: Initially `x` is an integer vector. Within the replacement call, we assign a double to the third element of `x`. So besides the new value of the third element also a type conversion (coercion) is triggered affecting whole vector:
    
    ```{r}
    # two copies
    x <- 1:3
    tracemem(x)
    
    x[[3]] <- 4
    
    # the same as 
    x <- 1:3
    tracemem(x)
    
    x <- 4L
    x <- as.double(x)
    
    # one copy
    x <- 1:3
    
    tracemem(x)
    x[[3]] <- 4L
    ```

3. __<span style="color:red">Q</span>__: Sketch out the relationship between the following objects:

    ```{r}
    a <- 1:10
    b <- list(a, a)
    c <- list(b, a, 1:10)
    ```
    
   __<span style="color:green">A</span>__: `a` contains a reference to an address with the value `1:10`. `b` contains a list of the same reference as `a` (twice). `c` contains a list of `b`, `a` (both containing the same reference three times) and a reference pointing to a different address containing the same value (`1:10`).

4. __<span style="color:red">Q</span>__: What happens when you run this code:

    ```{r}
    x <- list(1:10)
    x[[2]] <- x
    ```
    
   Draw a picture.

## Object size

1. __<span style="color:red">Q</span>__: In the following example, why are `object.size(y)` and `obj_size(y)`
   so radically different? Consult the documentation of `object.size()`.

    ```{r}
    y <- rep(list(runif(1e4)), 100)
    
    object.size(y)
    obj_size(y)
    ```
    
   __<span style="color:green">A</span>__: `object.size()` doesn't account for shared elements within lists.

2. __<span style="color:red">Q</span>__: Take the following list. Why is its size somewhat misleading?

    ```{r, return = FALSE}
    x <- list(mean, sd, var)
    # obj_size(x)
    #> 16,928 B
    ```

3. __<span style="color:red">Q</span>__: Predict the output of the following code:

    ```{r, eval = FALSE}
    # x <- 1:1e6
    # obj_size(x)
    # 
    # y <- list(x, x)
    # obj_size(y)
    # obj_size(x, y)
    # 
    # y[[1]][[1]] <- 10
    # obj_size(y)
    # obj_size(x, y)
    # 
    # y[[2]][[1]] <- 10
    # obj_size(y)
    # obj_size(x, y)
    ```
    
   __<span style="color:green">A</span>__: Since `lobstr::obj_size()` currently throws an error, we use `unclass(pryr::obj_size())` instead. 
    
   To predict the size of `x`, we first find out via `object_size(integer(0))` that an integer takes 40 B. For every element of the integer vector additionally 4 B are needed and R allocates memory in chunks of 2, so 8 B at a time. This can be verified for example via `sapply(1:100, function(x) pryr::object_size(integer(x)))`. Overall our prediction will result in 40 B + 1000000 * 4 B = 4000040 B:
    
    ```{r}
    x <- 1:1e6
    unclass(pryr::object_size(x))
    ```
    
   To predict the size of `y <- list(x, x)` we make usage of the fact that both list elements point to the same memory and hence are the same reference which means neither one needs additional memory. A list takes 40 B in memory and 8 B for each element (we can verify this in the same way as for integers). Overall our prediction will result in x (4000040 B) + list of length 2 (40 B + 16 B):
    
    ```{r}
    y <- list(x, x)
    unclass(pryr::object_size(y))
    ```
    
   Since `x` and `y` are names with bindings to objects that point to the same reference, no additional memory is needed and our prediction is the maximum memory of both objects (y; 4000040 B):
    
    ```{r}
    unclass(pryr::object_size(x, y))
    ```
    
   The next one gets a bit more tricky. Since the first element of `y` becomes different to `x`, a completely new object is created in memory. Hence 10 is of type double (which triggers a silent coercion), the new object will take more memory. A double needs 40 B + length * 8 B (overall 8000040 B). So we get: first element of `y` (8000040 B) + second element of `y` (`x`; 4000040 B) + list of length 2 (40 B + 16 B) = 12000136 B as our prediction:
    
    ```{r}
    y[[1]][[1]] <- 10
    unclass(pryr::object_size(y))
    ```
    
   Again all elements of `x` are shared within `y` (`x` is the second element of `y`). So the overall memory usage corresponds to `y`'s:
    
    ```{r}
    unclass(pryr::object_size(x, y))
    ```
    
   In the next example also the second element of `y` gets the same value as the first one. However, R does not now, that it is the same as the first element, so a new object is created taking the same amount of memory:
    
    ```{r}
    y[[2]][[1]] <- 10
    unclass(pryr::object_size(y))
    ```
    
   Now `x` and `y` don't share any values anymore (from Rs perspective) and their memory adds up:
    
    ```{r}
    unclass(pryr::object_size(x, y))
    ```

## Modify-in-place

1. __<span style="color:red">Q</span>__: Wrap the two methods for subtracting medians into two functions, then use the bench package to carefully compare their speeds. How does performance change as the number of columns increase?
    
   __<span style="color:green">A</span>__: 

   First, let's define a function to create some random data and a function to subtract the median from each column.

    ```{r}
    create_random_df <- function(nrow, ncol) {
      random_matrix <- matrix(runif(nrow * ncol), nrow = nrow)
      as.data.frame(random_matrix)
    }
    
    subtract_medians <- function(x, medians){
      for (i in seq_along(medians)) {
        x[[i]] <- x[[i]] - medians[[i]]
      }
      x
    }
    ```

   We can then profile the performance, by benchmarking `subtact_medians()` on data.frame- and list-input for a specified number columns.

    ```{r}
    compare_speed <- function(ncol){
      df_input   <- create_random_df(nrow = 1e4, ncol = ncol)
      list_input <- as.list(df_input)
      medians <- vapply(df_input, median, numeric(1))
      
      bench::mark(List         = subtract_medians(list_input, medians),
                  `Data Frame` = subtract_medians(df_input,   medians),
                  check = FALSE)
    }
    ```

   Then bench package allows us to run benchmark accros a grid of parameters easily. We will use it to slowly increase the number of columns of the random data.

    ```{r, warning=FALSE, message=FALSE}
    results <- bench::press(
      ncol = c(1, 10, 100, 500, 1000, 2000),
      compare_speed(ncol)
    )
    ```

   This shows, that the execution times for mean subtraction on data.frames increase exponentially when the number of columns of the input data increases. For linst-input the execution time increases linearly.

<!-- #TODO (optional) @taz: explain, why the computation time increases exponentially. (This has to do with the copying of the list, but I'm not sure, why this happens exponentially. I would think, that x get's copied proportional to the number of colums....) -->


    ```{r}
    library(ggplot2)
    results %>% 
      ggplot(aes(ncol, median, col = expression)) +
      geom_point() +
      labs(x = "Number of Columns", y = "Computation Time",
           color = "Input Data Structure",
           title = "Benchmark: Median Subtraction")
    ```

2. __<span style="color:red">Q</span>__: What happens if you attempt to use `tracemem()` on an environment?
